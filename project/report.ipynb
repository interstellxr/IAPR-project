{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f23e1c5",
   "metadata": {},
   "source": [
    "# [IAPR][iapr]: Final project - Chocolate Recognition\n",
    "\n",
    "\n",
    "**Moodle group ID:** *39*  \n",
    "**Kaggle challenge:** *`Classic`*\n",
    "**Kaggle team name (exact):** \"*xx*\"  \n",
    "\n",
    "**Author 1 (SCIPER):** *Léo Bruneau (xxxxx)*  \n",
    "**Author 2 (SCIPER):** *Louis Pivron (xxxxx)*  \n",
    "**Author 3 (SCIPER):** *Huckleberry Thums (xxxxx)*  \n",
    "\n",
    "**Due date:** 21.05.2025 (11:59 pm)\n",
    "\n",
    "\n",
    "## Key Submission Guidelines:\n",
    "- **Before submitting your notebook, <span style=\"color:red;\">rerun</span> it from scratch!** Go to: `Kernel` > `Restart & Run All`\n",
    "- **Only groups of three will be accepted**, except in exceptional circumstances.\n",
    "\n",
    "\n",
    "[iapr]: https://github.com/LTS5/iapr2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7939b0",
   "metadata": {},
   "source": [
    "## Justification of Design Choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db1b2c",
   "metadata": {},
   "source": [
    "### Sliding Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72069d28",
   "metadata": {},
   "source": [
    "One of the core parts of our pipeline is the sliding window detector. We came about this idea after having tried multiple classical object detection and segmentation methods. For example, we tried using region growing and contour detection, but these methods did not perform well on images with non-uniform backgrounds. Due to the clutter and the variety of backgrounds, it was difficult to foresee using contour or region based methods.\n",
    "\n",
    "Using a sliding window is much more robust to these variations. The idea is to take a window of a fixed size and slide it over the image with a fixed stride. For each position of the window, we compute the histogram of the pixels inside the window. This histogram is then compared to a set of histograms that we have pre-computed for each reference chocolate. The best-matching histogram is then used to determine the likelihood of the window containing a chocolate.\n",
    "\n",
    "This procedure leaves us with a heatmap showing the likelihood of each pixel being part of a chocolate. By thresholding, we can obtain a binary mask of the detections. The detections are then in the form of 'blobs' in the heatmap. To detect these blobs we use edge detection through the Laplacian of Gaussian (LoG) method. This method has input parameters that allow us to control the size of the blobs we want to detect, and how many by specifying a threshold.\n",
    "\n",
    "After the blobs have been detected, we can use the bounding boxes of these blobs to crop the original image and obtain the chocolate candidates. From these patches, we can then compute the features we want to use for classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b26c136",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8338a4",
   "metadata": {},
   "source": [
    "In our project we use a variety of features. Since we had trouble dealing with noisy backgrounds, our method does not rely on segmentations of the chocolates. So, the features we compute do not include any shape or contour based features. Instead, we use a combination of texture features, color features, and some basic statistics. Below we list the features we compute:\n",
    "\n",
    "**Color based features**: color statistics such as means and standard deviations in multiple color spaces (RGB, LAB) and color histograms in these same color spaces. \\\n",
    "**Texture based features**: Local Binary Patterns (LBP), Haralick GLCM features, and Gabor energy.\n",
    "\n",
    "Note that these features were computed on subdivisions of the segmentated reference chocolates and on the extracted patches of the train/test images. Subdivisions were done by splitting the masked reference images into a certain number of patches. The number of patches depends on the size of the reference chocolate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05a1f2a",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc23e5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe7bf9ad",
   "metadata": {},
   "source": [
    "## Technical Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76327eaa",
   "metadata": {},
   "source": [
    "### Reference Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a95061",
   "metadata": {},
   "source": [
    "Before segmenting and extracting features from the reference chocolates, we perform a series of preprocessing steps to isolate each chocolate in its image. Since each reference image contains a single chocolate on a relatively uniform background, segmentation can be approached with simple but effective methods.\n",
    "\n",
    "First, images are downsampled by a factor of four to reduce computational cost during edge detection and morphological operations. Edge detection is then performed using the Canny algorithm, which includes Gaussian smoothing to suppress noise and enhance true edges. This is what `compute_masks` does.\n",
    "\n",
    "For most reference chocolates, the Canny edges capture the chocolate contours well. However, in cases like Jelly White and Comtesse, the chocolates are low-contrast and similar in color to the background, making Canny alone insufficient. To address this, we apply the Hough ellipse transform to fit an ellipse to the detected edge points. This yields an approximate contour of the chocolate even when edges are faint or incomplete. The best ellipse is selected based on the accumulator score, and its perimeter is rasterized back into an edge mask.\n",
    "\n",
    "Once edges are obtained, either directly from Canny or via the fitted ellipse, we apply a morphological closing operation using a 7×7 kernel to seal any small gaps in the contour. We then fill internal holes to obtain a solid binary region, and apply erosion to shrink the mask slightly, ensuring that background pixels near the boundary are excluded.\n",
    "\n",
    "The result is a clean binary mask isolating the chocolate, which we use for two purposes: to extract visual features from the chocolate region, and to compute global color histograms that serve as templates for the sliding window detector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365b22c6",
   "metadata": {},
   "source": [
    "### Sliding Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b53946e",
   "metadata": {},
   "source": [
    "The sliding window detector is implemented in our `sliding_window_compare` function, which systematically scans the image with a fixed-size window (default: 64×64 pixels) and stride (default: 16 pixels). At each position, a patch is extracted and its color histogram is computed using the RGB channels, with histogram parameters tuned for sufficient granularity (16 bins per channel). We use a smoothed histogram via a Gaussian kernel to improve robustness to noise and small variations.\n",
    "\n",
    "This patch histogram is then compared to all reference chocolate histograms using the Bhattacharyya distance, a metric that quantifies similarity between probability distributions. The closest match is selected, and the inverse distance (1 − distance) is used as a similarity score. These scores are stored in a heatmap aligned with the image grid, representing local similarity to known chocolate types.\n",
    "\n",
    "This heatmap serves as the input for the next stage of the pipeline, where we apply blob detection via the Laplacian of Gaussian (LoG) to isolate likely chocolate candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0185a2c9",
   "metadata": {},
   "source": [
    "### Blob Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a68d76",
   "metadata": {},
   "source": [
    "The `compute_blobs` function identifies chocolate candidates by detecting local maxima in the heatmap produced by the sliding window detector. It uses the Laplacian of Gaussian (LoG) method via skimage.feature.blob_log, which is well-suited for detecting roughly circular blobs at multiple scales. The parameters min_sigma, max_sigma, and thr control the minimum and maximum expected blob size and the detection threshold, respectively.\n",
    "\n",
    "Once raw blobs are detected, their scale (σ) is converted to an approximate radius R=2σR=2​σ. However, LoG often produces multiple detections for a single object, especially when objects are large or span several overlapping regions. To address this, the function clusters nearby blobs using DBSCAN, treating detections within avg_R × 1.2 pixels of each other as belonging to the same chocolate candidate. For each cluster, the center is computed as the average of member centers, and the radius is set based on the selected merge_policy: the average radius, the maximum, or a fixed value (avg_R).\n",
    "\n",
    "The final output is a list of candidate detections, each represented as a circle (y, x, R) in heatmap coordinates, which can be mapped back to the original image for cropping or further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8f082f",
   "metadata": {},
   "source": [
    "### Patch Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48e429c",
   "metadata": {},
   "source": [
    "The `extract_crops` function extracts square patches from the original images, centered on the detected chocolate blobs. It takes as input a list of images, a list of blob detections for each image, and parameters defining the crop size, sliding window size, and stride.\n",
    "\n",
    "For each blob, the function first converts the blob coordinates from heatmap space to image space. This is done by scaling the coordinates by the stride and offsetting by half the window size to recover the center position in the original image. A square region of size crop_size × crop_size is then extracted around this center.\n",
    "\n",
    "To ensure consistency in input dimensions, the function pads the crop with zeros if it would otherwise extend beyond the image boundary or be smaller than the target size. This guarantees that all extracted patches are uniform in shape and suitable for downstream classification.\n",
    "\n",
    "The output is a list of lists of crops, where each sublist corresponds to the set of chocolate candidate regions extracted from a single image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9bd94b",
   "metadata": {},
   "source": [
    "### Training Data Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5556d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd90da75",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab16b18",
   "metadata": {},
   "source": [
    "At this point in the pipeline we have both the reference chocolate masks and labeled training patches. For the reference images, we subdivide the masks into patches of size 64x64 and compute the features on these patches. This is in a sense data augmentation, since we can use the same chocolate to compute multiple feature vectors and thus increasing the number of training samples. \n",
    "\n",
    "Considering all reference patches and training patches, this gives us a matrix of features of size 821 x 818.\n",
    "\n",
    "Using all features gives us feature vectors of length 818. Not all of these features may be useful, so we use PCA to reduce the dimensionality of the feature vectors. The number of components is set such that 95% of the variance is explained, which is a common practice. Before doing so it is necessary to standardize the features, since they are on different scales. We use the StandardScaler from sklearn to do this. The PCA is done using the PCA class from sklearn. The PCA is fitted on the training data (which includes the reference data) and then used to transform the future test data. This is done to ensure that the test data is transformed in the same way as the training data. The same can be said for the StandardScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cde63a",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59561d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e620ea2e",
   "metadata": {},
   "source": [
    "## Quantitative and Qualitative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed42a6",
   "metadata": {},
   "source": [
    "### Quantitative Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c148c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# > For the quantitative analysis, your Kaggle results, along with some intermediate results obtained throughout the project, should be sufficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3dd5c3",
   "metadata": {},
   "source": [
    "#### Main Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccc0bc1",
   "metadata": {},
   "source": [
    "The results of our pipeline and classification are in general very satisfactory considering we only use classical methods. We obtain a best F1-score of 0.85 after having optimized the various hyperparameters of the pipeline. As for the results on the public test set (those on Kaggle), we obtain a best F1 score of 0.73120 (with the same parameters as the ones used for the train set). We noticed that for some sets of parameters, the F1 score on the train set was higher than previous ones, but the F1 score on the public test set was lower. This is a sign of overfitting, which is something less present when using classical methods than when using deep learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3427e47",
   "metadata": {},
   "source": [
    "##### Other Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a0e4ab",
   "metadata": {},
   "source": [
    "Below is a plot of the cumulative explained variance as a function of the number of components. This shows that we can reduce the dimensionality of the feature vectors to 99% of the variance with less than 500 components. This is a good result, since it means that we can reduce the dimensionality of the feature vectors significantly without losing too much information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e1e543",
   "metadata": {},
   "source": [
    "<p float=\"center\">\n",
    "  <img src=\"images/pca.jpg\" width=\"800\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2870a95e",
   "metadata": {},
   "source": [
    "Here is the plot of the confusion matrix obtained for the training data (which, we should remind, includes the reference data). We can see that the model is able to classify most of the chocolates quite well (which can be seen as values close to 1 on the diagonal), with some exceptions. We can see that Creme Brulee is often classified as being Out-Of-Distribution (OOD), actually 30% of the time. This chocolate is quite different from the others, which is probably the cause of this. Though this is the worst result. \n",
    "\n",
    "We would like to point out the fact that the passion au lait chocolates are not well classified (relatively to the others). We saw this more intuitively in the heatmaps: for this chocolate, the blobs were often located on the edges of the chocolate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36adeea",
   "metadata": {},
   "source": [
    "<p float=\"center\">\n",
    "  <img src=\"images/confusion_matrix.jpg\" width=\"800\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e9afc",
   "metadata": {},
   "source": [
    "### Qualitative Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e850771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# > For qualitative analysis, we are looking for an interpretation of how your model works. Your model is not counting chocolates \"magically\"—it likely segments them internally and uses that information to compute useful descriptors.\n",
    "# > We expect you to show some examples of this internal segmentation (e.g., binary masks), and to demonstrate that the model can extract meaningful features.\n",
    "# > A helpful suggestion: you can extract the features and visualize them using a 2D PCA or t-SNE plot to assess whether the model learns discriminative representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb1537f",
   "metadata": {},
   "source": [
    "#### Reference Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9b62d6",
   "metadata": {},
   "source": [
    "Below we show the results of the segmentation on the reference images, after applying the Canny edge detector and the Hough ellipse transform when necessary, as well as the morphological operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f32ee87",
   "metadata": {},
   "source": [
    "<p float=\"center\">\n",
    "  <img src=\"images/masked_references.png\" width=\"1000\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba72c1b",
   "metadata": {},
   "source": [
    "#### Sliding Window Results: Heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585edfe",
   "metadata": {},
   "source": [
    "Below are some examples of nicely performing heatmaps.\n",
    "\n",
    "The image on the left is for a uniform background while the one on the right is for a noisy background.\n",
    "\n",
    "We can see that in both cases the histogram matching is able to produce clean heatmaps and the blob detection works perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b669012",
   "metadata": {},
   "source": [
    "<p float=\"left\">\n",
    "  <img src=\"images/good_heatmap_uniformbkgd.png\" width=\"600\" />\n",
    "  <img src=\"images/good_heatmap_noisybkgd.png\" width=\"600\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625dc17d",
   "metadata": {},
   "source": [
    "Below are some examples of poorly performing heatmaps on the two background types. \n",
    "\n",
    "On the left, corresponding to a uniform background, we can se that 6 out of the 8 chocolates are detected. The two missing chocolates are Noblesse. While the heatmap is non-zero on the latter, it is not enough to be detected by the blob detection. This is due to the fact that we had to use global parameters for the blob detection, which depends on the blob size and threshold. \n",
    "\n",
    "On the right, for the noisy background, the heatmap is much more noisy and the blobs are less well defined. The complex background makes it harder to detect the chocolates. This can be attributed to the fact that the sliding window is based off the color histograms. So if parts of the background are similar in color to the reference chocolates, the sliding window method will introduce false positives. In general we observed that these false positives showed up with a smaller intensity than the true positives, which is why we used a threshold to filter them out. Though this is not a perfect solution, it does help to reduce the number of false positives. Another problem stems from the fact that the sliding window is smaller than the chocolate size. So it is possible that the window detects the same chocolate multiple times. This is why we used the DBSCAN clustering method to group the detections. This is not perfect either, especially in the cases when the chocolates are very close to each other. In this case, the DBSCAN will group them together and we will only get one detection instead of two. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab78c08b",
   "metadata": {},
   "source": [
    "<p float=\"left\">\n",
    "  <img src=\"images/bad_heatmap_uniformbkgd.png\" width=\"600\" />\n",
    "  <img src=\"images/bad_heatmap_noisybkgd.png\" width=\"600\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e500dc",
   "metadata": {},
   "source": [
    "#### Patch Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ed0eb4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2287daa6",
   "metadata": {},
   "source": [
    "<p float=\"left\">\n",
    "  <img src=\"images/patches_image.png\" width=\"600\" />\n",
    "  <img src=\"images/patches.png\" width=\"1000\" />\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iapr_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
