{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f23e1c5",
   "metadata": {},
   "source": [
    "# [IAPR][iapr]: Final project - Chocolate Recognition\n",
    "\n",
    "\n",
    "**Moodle group ID:** *39*  \n",
    "**Kaggle challenge:** *`Classic`*\n",
    "**Kaggle team name (exact):** \"*xx*\"  \n",
    "\n",
    "**Author 1 (SCIPER):** *Léo Bruneau (xxxxx)*  \n",
    "**Author 2 (SCIPER):** *Louis Pivron (xxxxx)*  \n",
    "**Author 3 (SCIPER):** *Huckleberry Thums (xxxxx)*  \n",
    "\n",
    "**Due date:** 21.05.2025 (11:59 pm)\n",
    "\n",
    "\n",
    "## Key Submission Guidelines:\n",
    "- **Before submitting your notebook, <span style=\"color:red;\">rerun</span> it from scratch!** Go to: `Kernel` > `Restart & Run All`\n",
    "- **Only groups of three will be accepted**, except in exceptional circumstances.\n",
    "\n",
    "\n",
    "[iapr]: https://github.com/LTS5/iapr2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7939b0",
   "metadata": {},
   "source": [
    "## Justification of Design Choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b26c136",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8338a4",
   "metadata": {},
   "source": [
    "In our project we use a variety of features. Since we had trouble dealing with noisy backgrounds, our method does not rely on segmentations of the chocolates. So, the features we compute do not include any shape or contour based features. Instead, we use a combination of texture features, color features, and some basic statistics. Below we list the features we compute:\n",
    "\n",
    "Color based features: color statistics such as means and standard deviations in multiple color spaces (RGB, LAB) and color histograms in these same color spaces.\n",
    "Texture based features: Local Binary Patterns (LBP), Haralick GLCM features, and Gabor energy.\n",
    "\n",
    "Note that these features were computed on subdivisions of the segmentated reference chocolates and on the extracted patches of the train/test images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db1b2c",
   "metadata": {},
   "source": [
    "### Sliding Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72069d28",
   "metadata": {},
   "source": [
    "One of the core parts of our pipeline is the sliding window detector. We came about this idea after having tried multiple classical object detection and segmentation methods. For example, we tried using region growing and contour detection, but these methods did not perform well on images with non-uniform backgrounds. Due to the clutter and the variety of backgrounds, it was difficult to foresee using contour or region based methods.\n",
    "\n",
    "Using a sliding window is much more robust to these variations. The idea is to take a window of a fixed size and slide it over the image with a fixed stride. For each position of the window, we compute the histogram of the pixels inside the window. This histogram is then compared to a set of histograms that we have pre-computed for each reference chocolate. The best-matching histogram is then used to determine the likelihood of the window containing a chocolate. This is done by computing the Bhattacharyya distance between the histogram of the window and the histograms of the reference chocolates. The Bhattacharyya distance is a measure of similarity between two probability distributions.\n",
    "\n",
    "This procedure leaves us with a heatmap showing the likelihood of each pixel being part of a chocolate. By thresholding, we can obtain a binary mask of the detections. The detections are then in the form of 'blobs' in the heatmap. To detect these blobs we use edge detection through the Laplacian of Gaussian (LoG) method. This method has input parameters that allow us to control the size of the blobs we want to detect, and how many by specifying a threshold.\n",
    "\n",
    "After the blobs have been detected, we can use the bounding boxes of these blobs to crop the original image and obtain the chocolate candidates. From these patches, we can then compute the features we want to use for classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05a1f2a",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3559b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why we used a certain classifier\n",
    "# use of training set (fully labaled, by hand): supervised classification\n",
    "# optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7bf9ad",
   "metadata": {},
   "source": [
    "## Technical Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# > A schematic of your architecture is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44975ed1",
   "metadata": {},
   "source": [
    "### Data Preprocessing (general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93ea9e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parler ici du preprocessing fait sur les references et le training / test set (par exemple le downsampling)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76327eaa",
   "metadata": {},
   "source": [
    "### Reference Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aadc70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing specifique aux images de reference\n",
    "# Reference histograms used for later\n",
    "# Segmentation with masks and stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365b22c6",
   "metadata": {},
   "source": [
    "### Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b68f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How the sliding window works\n",
    "# Heatmap generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0185a2c9",
   "metadata": {},
   "source": [
    "### Blob Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce24366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoG method for blob detection, thresholding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8f082f",
   "metadata": {},
   "source": [
    "### Patch Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c207959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-explanatory: patches extracted from the detected blobs\n",
    "# resizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222d868",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0a44ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How we extract the features and details of which ones\n",
    "# Talk about PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cde63a",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa88f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how labeling was done\n",
    "# SVM\n",
    "# optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e620ea2e",
   "metadata": {},
   "source": [
    "## Quantitative and Qualitative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed42a6",
   "metadata": {},
   "source": [
    "### Quantitative Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b06111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# > For the quantitative analysis, your Kaggle results, along with some intermediate results obtained throughout the project, should be sufficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e9afc",
   "metadata": {},
   "source": [
    "### Qualitative Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e850771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# > For qualitative analysis, we are looking for an interpretation of how your model works. Your model is not counting chocolates \"magically\"—it likely segments them internally and uses that information to compute useful descriptors.\n",
    "# > We expect you to show some examples of this internal segmentation (e.g., binary masks), and to demonstrate that the model can extract meaningful features.\n",
    "# > A helpful suggestion: you can extract the features and visualize them using a 2D PCA or t-SNE plot to assess whether the model learns discriminative representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1420f610",
   "metadata": {},
   "source": [
    "## TA comments from the forum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcb6267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mais ducoup on peux upload des images des resultats intermediraires?\n",
    "# Par exemple la visualization de la classification, de la segmentation, ...\n",
    "# Ou on doit upload le code pour generer ces resultats?\n",
    "\n",
    "# Reponse sur le forum:\n",
    "# > Hello, Yes you can include the code that generates the figures in your notebook\n",
    "\n",
    "# Sauvegarder le classifier, scaler, etc. :\n",
    "# > As we will have to run your code in main.py, you can't use joblib. If you have a sklearn model, you can use pickle to save it. \n",
    "\n",
    "# > It is better to have a fully narrative description.\n",
    "# Donc en gros ne pas mettre de code dans le rapport\n",
    "\n",
    "# > Label for L1000780 is wrong: there are two jelly milk items instead of one jelly milk and one jelly black."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iapr_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
