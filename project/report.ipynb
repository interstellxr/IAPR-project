{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f23e1c5",
   "metadata": {},
   "source": [
    "# [IAPR][iapr]: Final project - Chocolate Recognition\n",
    "\n",
    "\n",
    "**Moodle group ID:** *39*  \n",
    "**Kaggle challenge:** *`Classic`*\n",
    "**Kaggle team name (exact):** \"*xx*\"  \n",
    "\n",
    "**Author 1 (SCIPER):** *Léo Bruneau (xxxxx)*  \n",
    "**Author 2 (SCIPER):** *Louis Pivron (xxxxx)*  \n",
    "**Author 3 (SCIPER):** *Huckleberry Thums (xxxxx)*  \n",
    "\n",
    "**Due date:** 21.05.2025 (11:59 pm)\n",
    "\n",
    "\n",
    "## Key Submission Guidelines:\n",
    "- **Before submitting your notebook, <span style=\"color:red;\">rerun</span> it from scratch!** Go to: `Kernel` > `Restart & Run All`\n",
    "- **Only groups of three will be accepted**, except in exceptional circumstances.\n",
    "\n",
    "\n",
    "[iapr]: https://github.com/LTS5/iapr2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7939b0",
   "metadata": {},
   "source": [
    "## Justification of Design Choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44975ed1",
   "metadata": {},
   "source": [
    "### Data Preprocessing (general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea9e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parler ici du preprocessing fait sur les references et le training / test set (par exemple le downsampling)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76327eaa",
   "metadata": {},
   "source": [
    "### Reference Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aadc70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing specifique aux images de reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ed876b",
   "metadata": {},
   "source": [
    "### Train/Test Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a39f73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing specifique aux images qu'on donne a la pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7bf9ad",
   "metadata": {},
   "source": [
    "## Technical Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# > A schematic of your architecture is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e620ea2e",
   "metadata": {},
   "source": [
    "## Quantitative and Qualitative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed42a6",
   "metadata": {},
   "source": [
    "### Quantitative Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b06111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# > For the quantitative analysis, your Kaggle results, along with some intermediate results obtained throughout the project, should be sufficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e9afc",
   "metadata": {},
   "source": [
    "### Qualitative Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e850771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# > For qualitative analysis, we are looking for an interpretation of how your model works. Your model is not counting chocolates \"magically\"—it likely segments them internally and uses that information to compute useful descriptors.\n",
    "# > We expect you to show some examples of this internal segmentation (e.g., binary masks), and to demonstrate that the model can extract meaningful features.\n",
    "# > A helpful suggestion: you can extract the features and visualize them using a 2D PCA or t-SNE plot to assess whether the model learns discriminative representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1420f610",
   "metadata": {},
   "source": [
    "## TA comments from the forum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcb6267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mais ducoup on peux upload des images des resultats intermediraires?\n",
    "# Par exemple la visualization de la classification, de la segmentation, ...\n",
    "# Ou on doit upload le code pour generer ces resultats?\n",
    "\n",
    "# Reponse sur le forum:\n",
    "# > Hello, Yes you can include the code that generates the figures in your notebook\n",
    "\n",
    "# Sauvegarder le classifier, scaler, etc. :\n",
    "# > As we will have to run your code in main.py, you can't use joblib. If you have a sklearn model, you can use pickle to save it. \n",
    "\n",
    "# > It is better to have a fully narrative description.\n",
    "# Donc en gros ne pas mettre de code dans le rapport\n",
    "\n",
    "# > Label for L1000780 is wrong: there are two jelly milk items instead of one jelly milk and one jelly black."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iapr_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
